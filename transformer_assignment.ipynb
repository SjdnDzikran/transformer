{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2OqYjR4nL7lS",
   "metadata": {
    "id": "2OqYjR4nL7lS"
   },
   "source": [
    "# Deep Learning Assignment: The Transformer\n",
    "\n",
    "### Instructions:\n",
    "1.  **File > Save a copy in Drive** to create your own editable version of this notebook.\n",
    "2.  Read the instructions in the Markdown cells.\n",
    "3.  Implement your code in the designated \"YOUR CODE HERE\" cells.\n",
    "4.  Answer the analysis questions in the Markdown cells at the end.\n",
    "5.  **Deliverable:** Share your completed Colab notebook.\n",
    "\n",
    "### Assignment Objectives\n",
    "* **Understand & Implement** the core \"heart\" of the Transformer: Multi-Head Attention.\n",
    "* **Learn** to debug a neural network by \"seeing\" what it \"sees\" (analyzing attention weights).\n",
    "* **Appreciate** the power of abstraction by using the Hugging Face library.\n",
    "* **Connect** the \"from scratch\" theory to the \"in-practice\" library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e_zYt03PL9vY",
   "metadata": {
    "id": "e_zYt03PL9vY"
   },
   "source": [
    "###  Task 0: Warm-up & Context\n",
    "\n",
    "**1. Watch a Video:**\n",
    "To understand the architecture you're about to build, watch a detailed visual explanation of the Transformer.\n",
    "* **Recommended Video:** **\"The Illustrated Transformer by Jay Alammar\"** (either the article or a video based on it) or [course from Huggingface](https://huggingface.co/learn/llm-course/en/chapter1/4).\n",
    "\n",
    "**2. Write a Reflection (Markdown Cell):**\n",
    "In the cell below, write a 1-paragraph reflection:\n",
    "* \"What is the key idea of 'self-attention'? Based on the video, why was this a significant change from models like RNNs and LSTMs?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E3g2YdOaMAMd",
   "metadata": {
    "id": "E3g2YdOaMAMd"
   },
   "source": [
    "**Reflection:** Jay Alammar's Illustrated Transformer helped me see how self-attention lets every token attend to the full sentence without the sequential bottleneck of RNNs. The query/key/value breakdown and scaled dot-product softmax made it clearer how the model learns to focus on syntactic roles (like subject vs. object) while still capturing long-range dependencies. I also appreciated how multi-head attention acts like several parallel lenses, each specializing in different patterns (positions, agreement, emphasis) and then recombining them. The residual connections, layer norm, and positional encodings felt less like implementation details and more like the glue that keeps deep stacks stable and order-aware. Overall, the architecture feels elegant: simple primitives plus good engineering to make training fast, parallel, and expressive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cI0963d7MCGq",
   "metadata": {
    "id": "cI0963d7MCGq"
   },
   "source": [
    "### Setup: Imports and Installs\n",
    "\n",
    "Run this cell to install and import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "k8vCj9y-MBg9",
   "metadata": {
    "id": "k8vCj9y-MBg9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets transformers torch\n",
    "!pip install -q nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "# For Part C\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Helper to set seeds for reproducible debugging\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W7O-Ld36MEC9",
   "metadata": {
    "id": "W7O-Ld36MEC9"
   },
   "source": [
    "### Part A: Build MultiHeadAttention From Scratch\n",
    "\n",
    "Your goal is to implement the `MultiHeadAttention` module.\n",
    "\n",
    "#### 1. Implement `ScaledDotProductAttention`\n",
    "\n",
    "This is the core function. Implement the formula:\n",
    "$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + \\text{mask}\\right)V$\n",
    "\n",
    "* Use `-1e9` for the mask value (a large negative number).\n",
    "* The function should return **both** the `output` and the attention `weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cI8uVlJ0MEoO",
   "metadata": {
    "id": "cI8uVlJ0MEoO"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Implements the Scaled Dot-Product Attention.\n",
    "    Args:\n",
    "        Q (torch.Tensor): Queries (batch_size, num_heads, seq_len_q, head_dim)\n",
    "        K (torch.Tensor): Keys (batch_size, num_heads, seq_len_k, head_dim)\n",
    "        V (torch.Tensor): Values (batch_size, num_heads, seq_len_v, head_dim)\n",
    "                         (seq_len_k and seq_len_v are the same)\n",
    "        mask (torch.Tensor, optional): Mask to apply. (batch_size, 1, 1, seq_len_k)\n",
    "    Returns:\n",
    "        output (torch.Tensor): The context vector.\n",
    "        attn_weights (torch.Tensor): The attention weights.\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3l3l7P8XMIpP",
   "metadata": {
    "id": "3l3l7P8XMIpP"
   },
   "source": [
    "#### 2. Implement `MultiHeadAttention`\n",
    "\n",
    "Now, create a module that uses your function to run multiple heads in parallel.\n",
    "\n",
    "**Crucial Instruction:** Your module's `forward` method must return **two** values:\n",
    "1.  `context_vector` (the final output of the module)\n",
    "2.  `attention_weights` (the weights from the scaled dot-product attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tE2Yg6k8MIsj",
   "metadata": {
    "id": "tE2Yg6k8MIsj"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by num_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, head_dim)\"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input (batch_size, seq_len, embed_dim)\n",
    "            mask (torch.Tensor, optional): Mask for attention.\n",
    "        Returns:\n",
    "            context_vector (torch.Tensor): Final output\n",
    "            attention_weights (torch.Tensor): Attention weights\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        Q = self.split_heads(self.W_q(x), batch_size)\n",
    "        K = self.split_heads(self.W_k(x), batch_size)\n",
    "        V = self.split_heads(self.W_v(x), batch_size)\n",
    "        context, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "        context_vector = self.W_o(context)\n",
    "        return context_vector, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e_bT-443MLdK",
   "metadata": {
    "id": "e_bT-443MLdK"
   },
   "source": [
    "#### 3. Test Your Module\n",
    "\n",
    "Run this cell. If your implementation is correct, it will run without errors and print the correct output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "W7e22N3LMM-K",
   "metadata": {
    "id": "W7e22N3LMM-K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part A: MultiHeadAttention Test Passed!\n",
      "Input shape: torch.Size([4, 10, 64])\n",
      "Output context shape: torch.Size([4, 10, 64])\n",
      "Output weights shape: torch.Size([4, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Unit Test\n",
    "set_seed(42)\n",
    "test_mha = MultiHeadAttention(embed_dim=64, num_heads=8).to(device)\n",
    "dummy_input = torch.rand(4, 10, 64).to(device)  # (batch_size, seq_len, embed_dim)\n",
    "context_vec, attn_weights = test_mha(dummy_input)\n",
    "\n",
    "# Check shapes\n",
    "assert context_vec.shape == (4, 10, 64), f\"Context vector shape is {context_vec.shape}\"\n",
    "assert attn_weights.shape == (4, 8, 10, 10), f\"Attn weights shape is {attn_weights.shape}\"\n",
    "\n",
    "print(\"Part A: MultiHeadAttention Test Passed!\")\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output context shape: {context_vec.shape}\")\n",
    "print(f\"Output weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6gJkG5p3MQf_",
   "metadata": {
    "id": "6gJkG5p3MQf_"
   },
   "source": [
    "### Part B: Task (Debugging)\n",
    "\n",
    "**Your Task:** I have provided a `BuggyTransformerEncoderLayer`. It uses your `MultiHeadAttention` module. This code **runs** but **fails to learn**.\n",
    "\n",
    "Your job is to:\n",
    "1.  Run the buggy training loop.\n",
    "2.  Use `torch.hooks` to \"spy\" on your `MultiHeadAttention` module.\n",
    "3.  Extract and plot the attention weights.\n",
    "4.  Analyze the plot and identify the bug in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7T4R-o1iMQjS",
   "metadata": {
    "id": "7T4R-o1iMQjS"
   },
   "source": [
    "#### 1. The Buggy Code (Read Only)\n",
    "\n",
    "Do not edit this cell. This is the broken code I am providing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4_1h3GOMQlc",
   "metadata": {
    "id": "e4_1h3GOMQlc"
   },
   "outputs": [],
   "source": [
    "class BuggyTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
    "        super(BuggyTransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        # We use your MultiHeadAttention module\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # --- This is the buggy part ---\n",
    "        # Find the one-line error in this forward pass.\n",
    "\n",
    "        attn_output, attn_weights = self.mha(x, mask)\n",
    "        x = self.dropout1(attn_output)\n",
    "        x = self.norm1(x)  # <--- HINT: Something is missing here!\n",
    "\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout2(ffn_output) # <--- And something is wrong here!\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x, attn_weights # Pass weights through for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L1T8-n1YMS_-",
   "metadata": {
    "id": "L1T8-n1YMS_-"
   },
   "source": [
    "#### 2. Data and Training Helpers (Read Only)\n",
    "\n",
    "This cell contains the helper functions to load data and run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4G6Yv_5CMTDE",
   "metadata": {
    "id": "4G6Yv_5CMTDE"
   },
   "outputs": [],
   "source": [
    "# Download NLTK data (if not already)\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "def get_nltk_data():\n",
    "    documents = [(list(movie_reviews.words(fileid)), category)\n",
    "                 for category in movie_reviews.categories()\n",
    "                 for fileid in movie_reviews.fileids(category)]\n",
    "    random.shuffle(documents)\n",
    "    return documents\n",
    "\n",
    "# Simple vocabulary and data processing\n",
    "class Vocab:\n",
    "    def __init__(self, documents):\n",
    "        all_words = [w.lower() for doc, cat in documents for w in doc]\n",
    "        self.word_freq = nltk.FreqDist(all_words)\n",
    "        self.vocab = {word: i+2 for i, (word, freq) in enumerate(self.word_freq.most_common(3000))}\n",
    "        self.vocab['<PAD>'] = 0\n",
    "        self.vocab['<UNK>'] = 1\n",
    "\n",
    "    def tokenize(self, doc, max_len=100):\n",
    "        tokens = [self.vocab.get(w.lower(), self.vocab['<UNK>']) for w in doc]\n",
    "        tokens = tokens[:max_len]\n",
    "        tokens = tokens + [self.vocab['<PAD>']] * (max_len - len(tokens))\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "def get_dataloaders(batch_size=32):\n",
    "    documents = get_nltk_data()\n",
    "    train_docs, test_docs = documents[:1800], documents[1800:]\n",
    "    vocab = Vocab(documents)\n",
    "\n",
    "    train_data = [(vocab.tokenize(doc), 1 if cat == 'pos' else 0) for doc, cat in train_docs]\n",
    "    test_data = [(vocab.tokenize(doc), 1 if cat == 'pos' else 0) for doc, cat in test_docs]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "    return train_loader, test_loader, len(vocab.vocab)\n",
    "\n",
    "# The full (buggy) classifier\n",
    "class BuggyClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ffn_dim):\n",
    "        super(BuggyClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # We use the BuggyEncoderLayer\n",
    "        self.encoder = BuggyTransformerEncoderLayer(embed_dim, num_heads, ffn_dim)\n",
    "        # self.pooler = nn.Linear(embed_dim, 1) # Simple pooler\n",
    "        self.classifier = nn.Linear(embed_dim, 2)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len=100)\n",
    "        padding_mask = (x == 0).unsqueeze(1).unsqueeze(2) # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "        x = self.embedding(x) * math.sqrt(self.embed_dim)\n",
    "        # Positional encoding is skipped to make the bug more obvious\n",
    "\n",
    "        x, attn_weights = self.encoder(x, mask=padding_mask)\n",
    "\n",
    "        # Pool by taking the first 100 tokens\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = x[:, :100 * self.embed_dim // self.embed_dim] # Hacky way to get (batch_size, 100)\n",
    "\n",
    "        # x = F.relu(self.pooler(x.view(x.size(0), self.embed_dim, 100))) # This is a mess\n",
    "        # x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits, attn_weights\n",
    "\n",
    "# Training Loop\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for data, labels in loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(data)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            logits, _ = model(data)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e_zL_57UMVPY",
   "metadata": {
    "id": "e_zL_57UMVPY"
   },
   "source": [
    "#### 3. Run the Buggy Training (and inspect the problem)\n",
    "\n",
    "This cell trains the buggy model. Notice how the accuracy is stuck around 50%? This means it's not learning, it's just guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T7jN6_mCMVwY",
   "metadata": {
    "id": "T7jN6_mCMVwY"
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "train_loader, test_loader, vocab_size = get_dataloaders()\n",
    "buggy_model = BuggyClassifier(vocab_size, 64, 8, 128).to(device)\n",
    "optimizer = torch.optim.Adam(buggy_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"--- Training Buggy Model ---\")\n",
    "for epoch in range(3):\n",
    "    train(buggy_model, train_loader, optimizer, criterion)\n",
    "    acc = evaluate(buggy_model, test_loader)\n",
    "    print(f\"Epoch {epoch+1}, Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L88f3I01MXwG",
   "metadata": {
    "id": "L88f3I01MXwG"
   },
   "source": [
    "#### 4. Your Task: Debug with Hooks\n",
    "\n",
    "Fill in the code below to:\n",
    "1.  Instantiate a new `BuggyClassifier`.\n",
    "2.  Register a `forward_hook` on the `MultiHeadAttention` module inside it.\n",
    "3.  The hook should save the `attn_weights` (the 2nd item returned by your MHA's forward pass) into the `attention_storage` list.\n",
    "4.  Run training for **one** epoch to populate the `attention_storage`.\n",
    "5.  Plot the attention weights for the first head of the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e_b10LMX1E",
   "metadata": {
    "id": "87e_b10LMX1E"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "set_seed(42)\n",
    "# 1. Instantiate a new model, optimizer, and criterion\n",
    "debug_model = BuggyClassifier(vocab_size, 64, 8, 128).to(device)\n",
    "optimizer = torch.optim.Adam(debug_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 2. Set up the hook\n",
    "attention_storage = []\n",
    "def hook_fn(module, input, output):\n",
    "    # output is (context_vector, attention_weights)\n",
    "    # We want to save the attention_weights\n",
    "\n",
    "    # YOUR CODE HERE (1 line)\n",
    "\n",
    "\n",
    "# 3. Register the hook on the MHA module\n",
    "#    (Hint: debug_model.encoder.mha)\n",
    "# YOUR CODE HERE (1 line)\n",
    "\n",
    "\n",
    "\n",
    "# 4. Run training for one epoch\n",
    "print(\"Running one epoch of training to capture weights...\")\n",
    "train(debug_model, train_loader, optimizer, criterion)\n",
    "print(\"Training done.\")\n",
    "\n",
    "\n",
    "# 5. Plot the attention weights\n",
    "if attention_storage:\n",
    "    print(\"Plotting attention weights from the first batch...\")\n",
    "    # Get weights from the first batch: (batch_size, num_heads, seq_len, seq_len)\n",
    "    first_batch_weights = attention_storage[0]\n",
    "\n",
    "    # Get weights for the first item in the batch, first head\n",
    "    # (seq_len, seq_len)\n",
    "    weights_to_plot = first_batch_weights[0, 0, :, :].numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(weights_to_plot)\n",
    "    plt.title(\"Attention Weights (First Batch, First Head)\")\n",
    "    plt.xlabel(\"Key (Token Position)\")\n",
    "    plt.ylabel(\"Query (Token Position)\")\n",
    "    plt.colorbar(label=\"Attention Weight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Hook failed to capture attention weights. Check your hook implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J1c3WwT3MZzH",
   "metadata": {
    "id": "J1c3WwT3MZzH"
   },
   "source": [
    "#### 5. Analysis & The Bug\n",
    "\n",
    "Based on the plot above (which will likely look very strange or random, not focused), answer the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eJjA-gP3MZyW",
   "metadata": {
    "id": "eJjA-gP3MZyW"
   },
   "source": [
    "<div style=\"border: 2px dashed #ccc; padding: 10px; background-color: #f9f9f9;\">\n",
    "    \n",
    "<b>YOUR ANALYSIS HERE:</b>\n",
    "\n",
    "**1. Analyze the Plot:**\n",
    "<i>(Double-click to edit. Describe what you see in the plot. Is the attention focused? Is it random? Is it attending to padding tokens? What does this tell you about the model's ability to learn?)</i>\n",
    "\n",
    "**2. Identify the Bug:**\n",
    "<i>(Look at the <code>BuggyTransformerEncoderLayer</code> code. There are two one-line bugs related to residual connections and layer norm. Find them.)</i>\n",
    "\n",
    "* **Bug 1 (Line 23):**\n",
    "* **Bug 2 (Line 26):**\n",
    "\n",
    "**3. Explain the Bug:**\n",
    "<i>(Why do these bugs (especially the missing residual connection) cause the model to fail and produce the attention plot you see? What is a residual connection <i>for</i>?)</i>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kFhYgP0MMbxj",
   "metadata": {
    "id": "kFhYgP0MMbxj"
   },
   "source": [
    "### Part C: Use the Abstraction (Hugging Face)\n",
    "\n",
    "Now, let's solve the *same problem* (NLTK movie reviews) with Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7LzP-oBBMc03",
   "metadata": {
    "id": "7LzP-oBBMc03"
   },
   "outputs": [],
   "source": [
    "# 1. Load the NLTK data into a simple list format\n",
    "set_seed(42)\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "def get_hf_dataset():\n",
    "    documents = []\n",
    "    labels = []\n",
    "    for fileid in movie_reviews.fileids('pos'):\n",
    "        documents.append(movie_reviews.raw(fileid))\n",
    "        labels.append(1) # 'pos'\n",
    "    for fileid in movie_reviews.fileids('neg'):\n",
    "        documents.append(movie_reviews.raw(fileid))\n",
    "        labels.append(0) # 'neg'\n",
    "\n",
    "    # Create a datasets.Dataset\n",
    "    from datasets import Dataset, train_test_split_dict\n",
    "    dataset = Dataset.from_dict({'text': documents, 'label': labels})\n",
    "\n",
    "    # Split into train/test\n",
    "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    return dataset['train'], dataset['test']\n",
    "\n",
    "train_dataset, test_dataset = get_hf_dataset()\n",
    "print(f\"Hugging Face train dataset: {train_dataset}\")\n",
    "\n",
    "# 2. Load Tokenizer and Model\n",
    "# YOUR CODE HERE\n",
    "# Use \"distilbert-base-uncased\" as the model_name\n",
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load the model (for sequence classification, 2 labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "# 3. Create a preprocessing function\n",
    "def tokenize_fn(batch):\n",
    "    # YOUR CODE HERE\n",
    "    # Use the tokenizer on the 'text' field.\n",
    "    # Make sure to truncate!\n",
    "    return # return tokenizer\n",
    "\n",
    "# 4. Tokenize the datasets\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# 5. Set up Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,  # 1 epoch is fine for this demo\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\", # Disable wandb/tensorboard reporting\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tok,\n",
    "    eval_dataset=test_dataset_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 6. Train!\n",
    "print(\"\\n--- Training Hugging Face Model ---\")\n",
    "trainer.train()\n",
    "\n",
    "# 7. Evaluate\n",
    "print(\"\\n--- Evaluating Hugging Face Model ---\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Final Test Accuracy (Hugging Face): {eval_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e_zYt03PL9vY-copy",
   "metadata": {
    "id": "e_zYt03PL9vY-copy"
   },
   "source": [
    "### Part D: Final Analysis & Reflection\n",
    "\n",
    "Answer the final questions in this Markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N-Jp2zKWMf3k",
   "metadata": {
    "id": "N-Jp2zKWMf3k"
   },
   "source": [
    "<div style=\"border: 2px dashed #ccc; padding: 10px; background-color: #f9f9f9;\">\n",
    "    \n",
    "<b>YOUR FINAL ANALYSIS:</b>\n",
    "\n",
    "**1. Final Accuracy:**\n",
    "<i>(What was the final test accuracy you achieved in Part C with Hugging Face?)</i>\n",
    "\n",
    "\n",
    "**2. Debugging vs. Building:**\n",
    "<i>(What did you learn from the <i>debugging</i> task in Part B that you wouldn't have learned from just <i>implementing</i> in Part A?)</i>\n",
    "\n",
    "\n",
    "**3. Connecting the Pieces:**\n",
    "<i>(In Part A, you built <i>one</i> `MultiHeadAttention` module. The `distilbert-base-uncased` model you used in Part C has <b>6 layers</b> and <b>12 attention heads</b> (per layer). This means it's using <b>6</b> of the <i>type</i> of module you built, and each one is 12-headed. How does this make you feel about the complexity of modern models?)</i>\n",
    "\n",
    "\n",
    "**4. Final Thought:**\n",
    "<i>(Why is it valuable to build a component like `MultiHeadAttention` from scratch, even if you will almost always use a library like Hugging Face in practice?)</i>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
